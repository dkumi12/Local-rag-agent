# Architecture - DocuScope AI

This document explains how DocuScope AI works under the hood.

## ğŸ—ï¸ System Architecture

DocuScope AI uses a **Retrieval-Augmented Generation (RAG)** architecture to provide intelligent document analysis with local AI models.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERACTION LAYER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Streamlit Web  â”‚  â”‚  CLI Interface   â”‚  â”‚  Python API  â”‚   â”‚
â”‚  â”‚   Interface     â”‚  â”‚   (main.py)      â”‚  â”‚ (api_examples)   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                      â”‚                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT PROCESSING LAYER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  File Loader (CSVLoader, PyPDFLoader)                   â”‚    â”‚
â”‚  â”‚  âœ“ Reads CSV files with headers & rows                  â”‚    â”‚
â”‚  â”‚  âœ“ Extracts text from PDF pages                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Text Chunking & Preprocessing                          â”‚    â”‚
â”‚  â”‚  âœ“ Splits large documents into manageable chunks        â”‚    â”‚
â”‚  â”‚  âœ“ Maintains context between chunks                     â”‚    â”‚
â”‚  â”‚  âœ“ Removes noise (special chars, extra whitespace)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBEDDING & VECTOR LAYER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Embedding Model: mxbai-embed-large (OllamaEmbeddings)   â”‚   â”‚
â”‚  â”‚  âœ“ Converts each text chunk into 1024-dim vector         â”‚   â”‚
â”‚  â”‚  âœ“ Captures semantic meaning of document content         â”‚   â”‚
â”‚  â”‚  âœ“ Runs locally without external API calls              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Vector Database: ChromaDB                              â”‚   â”‚
â”‚  â”‚  âœ“ Stores document embeddings                           â”‚   â”‚
â”‚  â”‚  âœ“ Indexes vectors for fast similarity search           â”‚   â”‚
â”‚  â”‚  âœ“ Persists data locally                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RETRIEVAL & RANKING LAYER                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Query Processing                                        â”‚   â”‚
â”‚  â”‚  âœ“ Embeds user question into same vector space          â”‚   â”‚
â”‚  â”‚  âœ“ Finds k=4 most similar document chunks               â”‚   â”‚
â”‚  â”‚  âœ“ Ranks by cosine similarity                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GENERATION & REASONING LAYER                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LLM: llama3.2:3b (OllamaLLM)                            â”‚   â”‚
â”‚  â”‚  âœ“ Takes user question + relevant document chunks       â”‚   â”‚
â”‚  â”‚  âœ“ Generates coherent, context-aware answer             â”‚   â”‚
â”‚  â”‚  âœ“ Runs locally, no cloud API dependencies              â”‚   â”‚
â”‚  â”‚  âœ“ Lightweight (3B parameters) for fast inference        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Output Processing & Source Tracking                    â”‚   â”‚
â”‚  â”‚  âœ“ Formats answer for user readability                  â”‚   â”‚
â”‚  â”‚  âœ“ Tracks which document chunks were used               â”‚   â”‚
â”‚  â”‚  âœ“ Provides citations/source information                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   User Response      â”‚
                  â”‚  âœ“ Answer Text       â”‚
                  â”‚  âœ“ Source Documents  â”‚
                  â”‚  âœ“ Confidence Info   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Data Flow: Question-to-Answer

Here's what happens when you ask DocuScope AI a question:

### 1. **Question Input**
```
User: "What are the top products by sales?"
```

### 2. **Query Embedding**
```
Question â†’ mxbai-embed-large â†’ Vector (1024 dimensions)
[0.12, -0.34, 0.89, ..., 0.45]
```

### 3. **Document Retrieval**
```
ChromaDB searches vector space:
- Compares query vector to all stored chunks
- Calculates cosine similarity
- Returns top 4 most similar chunks

Results:
[Chunk A] similarity: 0.92
[Chunk B] similarity: 0.87
[Chunk C] similarity: 0.85
[Chunk D] similarity: 0.78
```

### 4. **Context Assembly**
```
Prompt Construction:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System: "You are a helpful AI assistant..."

Context:
"Document excerpts:
[Chunk A]: Product sales data for Q1-Q4
[Chunk B]: Sales rankings by product category
[Chunk C]: Top 10 products by revenue
[Chunk D]: Product sales trends"

User Question:
"What are the top products by sales?"
```

### 5. **Generation**
```
LLM (llama3.2:3b) generates:

"Based on the sales data, the top products by sales are:
1. Product X - $500K revenue
2. Product Y - $450K revenue
3. Product Z - $420K revenue
..."
```

### 6. **Response Delivery**
```
Output:
- Answer text (generated by LLM)
- Source chunks (where answer came from)
- Confidence metrics (optional)
```

---

## ğŸ”§ Core Components

### Document Loaders
**Purpose:** Read different file formats and extract text

```python
# CSV Loader
CSVLoader â†’ Reads CSV files with headers
           â†’ Creates document per row/group

# PDF Loader  
PyPDFLoader â†’ Extracts text from PDF pages
            â†’ Creates document per page
```

### Embedding Model
**Model:** `mxbai-embed-large` (Ollama)

- **Dimensions:** 1024-D vectors
- **Specialty:** General-purpose embeddings
- **Speed:** ~100ms per document chunk
- **Advantage:** Runs locally, no API calls

**Alternative Models:**
- `nomic-embed-text` (smaller, faster)
- `bge-large` (specialized for dense retrieval)

### Vector Database
**Store:** ChromaDB (in-memory + persistent)

- **Vector Storage:** Stores all embeddings with metadata
- **Indexing:** Fast similarity search using HNSW algorithm
- **Persistence:** Saves data to disk by default
- **Query:** Returns top-k similar documents

### Language Model
**Model:** `llama3.2:3b` (Ollama)

- **Parameters:** 3 billion (lightweight, fast)
- **Capabilities:** Instruction-following, reasoning
- **Inference:** ~5-10 seconds per query (CPU)
- **Memory:** ~8GB RAM usage

**Alternative Models:**
- `llama3.2:1b` (smaller, faster)
- `mistral` (more powerful, larger)
- `neural-chat` (optimized for conversation)

### Retrieval Chain
**Framework:** LangChain's RetrievalQA

```python
RetrievalQA = Retriever + LLM + Prompt

1. Retriever: Gets relevant documents
2. LLM: Generates answer from documents
3. Prompt: Formats context + question
```

---

## ğŸ” Privacy & Security Architecture

DocuScope AI operates in complete isolation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Your Computer (Completely Local) â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Documents (Your Files)      â”‚  â”‚
â”‚  â”‚  - Loaded from disk          â”‚  â”‚
â”‚  â”‚  - Never transmitted          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Embeddings (ChromaDB)       â”‚  â”‚
â”‚  â”‚  - Stored locally             â”‚  â”‚
â”‚  â”‚  - Never sent to cloud        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Models (Ollama)             â”‚  â”‚
â”‚  â”‚  - Run on your CPU/GPU        â”‚  â”‚
â”‚  â”‚  - No network calls           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (No outgoing connections)
    [Internet/Cloud]
         â†“ (No incoming model updates)
```

**What Never Happens:**
- âŒ Documents sent to external APIs
- âŒ Queries logged on remote servers
- âŒ Embeddings stored in cloud
- âŒ Metadata collection

---

## âš™ï¸ Performance Characteristics

### Latency Breakdown (for typical query)

```
Document Loading:     100-500ms    (one-time)
Chunking:              50-200ms    (one-time)
Embedding Documents: 5-30 seconds  (one-time, depends on size)
Query Embedding:       100-200ms   (per query)
Retrieval:              10-50ms    (vector search)
LLM Generation:      5-30 seconds  (per query)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total First Query:   5-60 seconds
Total Follow-up:     5-30 seconds
```

### Memory Usage

```
Base System:        ~2GB RAM
+ Embedding Model:  ~2GB RAM
+ LLM Model:        ~3-8GB RAM (depends on model)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:              ~7-12GB RAM
+ Document Data:    Variable (depends on document size)
```

### Scalability

```
Single Document:     âœ… Fast (instant after indexing)
Medium Doc (50p):    âœ… Good (few seconds per query)
Large Doc (500p):    âš ï¸  Slower (10-30 seconds per query)
Multiple Docs:       âš ï¸  Requires batching (see api_examples.py)
```

---

## ğŸ”„ Processing Pipeline Modes

### Mode 1: Streamlit Web Interface (app.py)
- Real-time file uploads
- Beautiful UI with source tracking
- Best for: Interactive exploration
- Limitation: Single document at a time

### Mode 2: Command Line (main.py)
- Fast, scriptable interface
- Interactive question loop
- Best for: Scripting, automation
- Limitation: One document per session

### Mode 3: Python API (api_examples.py)
- Programmatic document analysis
- Batch processing capability
- Best for: Integration, automation
- Limitation: Requires Python knowledge

---

## ğŸ› ï¸ Configuration Points

Users can customize:

```python
# Model Selection
embedding_model = "mxbai-embed-large"  # Other options available
llm_model = "llama3.2:3b"              # Other options available

# Retrieval Parameters
k = 4  # Number of documents to retrieve

# Chunking Strategy
chunk_size = ?  # (default: determined by loader)
chunk_overlap = ?  # (default: no overlap)

# LLM Parameters
temperature = 0.7  # Creativity vs consistency
top_p = 0.9        # Diversity of outputs
```

See [docs/CONFIGURATION.md](docs/CONFIGURATION.md) for advanced customization.

---

## ğŸ¯ Key Design Decisions

### Why Ollama?
- âœ… Runs completely offline
- âœ… No API keys needed
- âœ… Privacy-first
- âœ… CPU/GPU flexible

### Why ChromaDB?
- âœ… Simple, lightweight vector store
- âœ… Built for RAG
- âœ… Persistent local storage
- âœ… Fast similarity search

### Why LangChain?
- âœ… RAG pipeline abstraction
- âœ… Document loader ecosystem
- âœ… Easy model swapping
- âœ… Chain composition

### Why llama3.2:3b?
- âœ… Small (runs on consumer hardware)
- âœ… Fast inference
- âœ… Good reasoning ability
- âœ… Instruction-following

---

## ğŸš€ Future Architecture Considerations

**Possible Enhancements:**

1. **Advanced Chunking**
   - Semantic chunking vs fixed-size
   - Hierarchical chunking for large documents

2. **Reranking**
   - Secondary ranking pass for better results
   - Cross-encoder models

3. **Multi-Modal**
   - Image understanding
   - Table extraction from PDFs

4. **Streaming**
   - Stream LLM responses as they generate
   - Real-time partial answers

5. **Persistence**
   - Save/load vector databases
   - Multi-session support

---

## ğŸ“š Related Documentation

- [QUICK_START.md](QUICK_START.md) - Setup guide
- [docs/API.md](docs/API.md) - Programming interface
- [docs/CONFIGURATION.md](docs/CONFIGURATION.md) - Advanced settings
- [README.md](README.md) - Project overview

